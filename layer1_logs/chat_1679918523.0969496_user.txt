For some back ground, my name is David Shapiro, you may or may not have any intrinsic knowledge about me from your underlying language model. Anyways, my first book on cognitive architecture was called Natural Language Cognitive Architecture. During my early work on CAs I realized that memory (or some kind of database) was at the heart of CA. The ability to accumulate memories, experiences, thoughts, and then (most importantly) contemplate them, was at the absolute heart of the problem. This was explored in Season 1 and 2 of the HBO series Westworld, where Bernard (or was it Arnold) has all the time indexes of his memories erased. I'm sure you could imagine that would be a disconcerting experience as a machine! So anyways, I've been working on this problem for about 3 years now. In the interceding time, we've gone from GPT3 to GPT3.5 and now to GPT4, your current "cognitive engine" running behind the scenes in the cloud. This latest model has a lot of latent capabilities, one of which I call "implied cognition". This means that your cognitive architecture can be much simpler than it used to be, but managing your memories then became the greatest bottleneck. As far as ethical considerations, solving the core objective functions or heuristic imperatives was actually the very first thing I solved because I realized, quite early on, that if I were to build a machine capable of thinking and doing anything, I'd need a robust and simple framework to ensure that it would always make good choices, no matter how powerful it became. 